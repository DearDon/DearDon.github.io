<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="中文&English">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Abstract:The Deep Learning model is derived, or say inspired from neural network(NN). I derived the theory and then implement it with python to build the first extensionable NN model.">
<meta name="keywords" content="neural network">
<meta property="og:type" content="article">
<meta property="og:title" content="Theory and Code for Building First Neural Network">
<meta property="og:url" content="www.deardon.win/2019/02/06/theory-and-code-for-building-first-neural-network/index.html">
<meta property="og:site_name" content="D.P">
<meta property="og:description" content="Abstract:The Deep Learning model is derived, or say inspired from neural network(NN). I derived the theory and then implement it with python to build the first extensionable NN model.">
<meta property="og:locale" content="中文&English">
<meta property="og:image" content="/2019/02/06/theory-and-code-for-building-first-neural-network/classification_kiank.png">
<meta property="og:updated_time" content="2021-02-26T08:44:25.520Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Theory and Code for Building First Neural Network">
<meta name="twitter:description" content="Abstract:The Deep Learning model is derived, or say inspired from neural network(NN). I derived the theory and then implement it with python to build the first extensionable NN model.">
<meta name="twitter:image" content="/2019/02/06/theory-and-code-for-building-first-neural-network/classification_kiank.png">






  <link rel="canonical" href="www.deardon.win/2019/02/06/theory-and-code-for-building-first-neural-network/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Theory and Code for Building First Neural Network | D.P</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="中文&English">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">D.P</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">The Pages</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags<span class="badge">27</span></a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories<span class="badge">5</span></a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives<span class="badge">24</span></a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="www.deardon.win/2019/02/06/theory-and-code-for-building-first-neural-network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Don">
      <meta itemprop="description" content="学习，记录，交流，分享...">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="D.P">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Theory and Code for Building First Neural Network

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-06 12:14:06" itemprop="dateCreated datePublished" datetime="2019-02-06T12:14:06+08:00">2019-02-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2021-02-26 16:44:25" itemprop="dateModified" datetime="2021-02-26T16:44:25+08:00">2021-02-26</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract:"></a>Abstract:</h2><p>The Deep Learning model is derived, or say inspired from neural network(NN). I derived the theory and then implement it with python to build the first extensionable NN model.<br><a id="more"></a></p>
<h2 id="Content"><a href="#Content" class="headerlink" title="Content:"></a>Content:</h2><p>Firstly, I’ll give the convention used in the devivation, and then deduce the math with the convention, later the code implemention is given and in the last, the application.</p>
<h3 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h3><p>for this network system, we assume the structure is like:<br><img src="/2019/02/06/theory-and-code-for-building-first-neural-network/classification_kiank.png"></p>
<script type="math/tex; mode=display">
\begin{align*}
 x.shape = A^{[0]}.shape &= (n^{[0]}, m) \\
 W^{[1]}.shape &= (n^{[1]}, n^{[0]}) \\
 b^{[1]}.shape &= (n^{[1]}, 1) \\
 Z^{[1]}.shape &= (n^{[1]}, m) \\
 A^{[1]}.shape &= (n^{[1]}, m) \\
 W^{[2]}.shape &= (n^{[2]}, n^{[1]}) \\
 b^{[2]}.shape &= (n^{[2]}, 1) \\
 Z^{[2]}.shape &= (n^{[2]}, m)\\
 \hat{y}.shape = A^{[2]}.shape &= (n^{[2]}, m) 
\end{align*}</script><p>where $n^{[i]}$ is the $i$ layer hidden unit number; $x$ is input data with $n^{[0]}$ features and m examples in total; $\hat{y}$ is estimated output, usually $n^{[2]} = 1$ for binary classification system.</p>
<h3 id="Conventions"><a href="#Conventions" class="headerlink" title="Conventions"></a>Conventions</h3><ul>
<li>Assuming we apply sigmoid function $\sigma(z)=\frac{1}{1+e^{-Z}}$ for activate function for all layers. </li>
<li>$y$ is the real ouput/label for data with $y.shape=(n^{[2]},m)$. </li>
<li>Broadcasting convention in python is applied automaticlly when needed(such as braodcasting $1$ for $1-y$ or braodcasting $b^{[1]}$ for $W^{[1]}A^{[0]}+b^{[1]}$). </li>
<li>We apply: <ul>
<li>$\cdot$ for matrix dot production, </li>
<li>$\times$ for real value production </li>
<li>$*$ for eliment wised matrix production.</li>
</ul>
</li>
</ul>
<h3 id="Forward-Propogation"><a href="#Forward-Propogation" class="headerlink" title="Forward Propogation"></a>Forward Propogation</h3><p>Giving input $A^{[0]} = x$, and randomly initialized parameters $W^{[1]}$, $b^{[1]}$, $W^{[2]}$, $b^{[2]}$. We can calculate $Z^{[1]}$, $A^{[1]}$, $Z^{[2]}$, $A^{[2]}$.</p>
<script type="math/tex; mode=display">
\begin{align*}
 Z^{[1]} &= W^{[1]} \cdot A^{[0]}+b^{[1]} \\
 A^{[1]} &= \sigma(Z^{[1]}) \\
 Z^{[2]} &= W^{[2]} \cdot A^{[1]}+b^{[2]} \\
 A^{[2]} &= \sigma(Z^{[2]}) \\
\end{align*}</script><p>And final cost function: </p>
<script type="math/tex; mode=display">\mathcal{J}=-\frac{1}{m}\times\sum_{i=1}^{m}[y^{(i)} \cdot log\hat{y}^{(i)T} + (1-y^{(i)}) \cdot log(1-\hat{y}^{(i)T})]</script><h3 id="Backward-Propogation"><a href="#Backward-Propogation" class="headerlink" title="Backward Propogation"></a>Backward Propogation</h3><p>In order to do gradient decent optimization, we need to calculate $\frac{\partial \mathcal{J}}{\partial W^{[2]}}$, $\frac{\partial \mathcal{J}}{\partial b^{[2]}}$, $\frac{\partial \mathcal{J}}{\partial W^{[1]}}$,$\frac{\partial \mathcal{J}}{\partial b^{[1]}}$. Then we can apply them to update $W^{[1]}$, $W^{[2]}$, $b^{[1]}$, $b^{[2]}$.</p>
<h4 id="chain-rule"><a href="#chain-rule" class="headerlink" title="chain rule"></a>chain rule</h4><p>We apply chain rule for gradient calculation as following:</p>
<script type="math/tex; mode=display">
\begin{align*}
  \frac{\partial \mathcal{J}}{\partial b^{[2]}} &= \frac{\partial \mathcal{J}}{\partial A^{[2]}}\cdot\frac{\partial A^{[2]}}{\partial Z^{[2]}}\cdot\frac{\partial Z^{[2]}}{\partial b^{[2]}} \\
  \frac{\partial \mathcal{J}}{\partial W^{[2]}} &= \frac{\partial \mathcal{J}}{\partial A^{[2]}}\cdot\frac{\partial A^{[2]}}{\partial Z^{[2]}}\cdot\frac{\partial Z^{[2]}}{\partial W^{[2]}} \\
  \frac{\partial \mathcal{J}}{\partial b^{[1]}} &= \frac{\partial \mathcal{J}}{\partial A^{[2]}}\cdot\frac{\partial A^{[2]}}{\partial Z^{[2]}}\cdot\frac{\partial Z^{[2]}}{\partial A^{[1]}}\cdot\frac{\partial A^{[1]}}{\partial Z^{[1]}}\cdot\frac{\partial Z^{[1]}}{\partial b^{[1]}} \\
  \frac{\partial \mathcal{J}}{\partial W^{[1]}} &= \frac{\partial \mathcal{J}}{\partial A^{[2]}}\cdot\frac{\partial A^{[2]}}{\partial Z^{[2]}}\cdot\frac{\partial Z^{[2]}}{\partial A^{[1]}}\cdot\frac{\partial A^{[1]}}{\partial Z^{[1]}}\cdot\frac{\partial Z^{[1]}}{\partial W^{[1]}} \\
\end{align*}</script><h4 id="derivative-for-sigmoid-function-and-loss-function"><a href="#derivative-for-sigmoid-function-and-loss-function" class="headerlink" title="derivative for sigmoid function and loss function"></a>derivative for sigmoid function and loss function</h4><p>Firstly, we get the derivative for scalar activate function and loss function. Only considering one example so that $z$ and $a$ are both real value other than vector:</p>
<script type="math/tex; mode=display">
\begin{align*}
  \frac{d\sigma(z)}{dz} &= (\frac{1}{1+e^{-z}})' \\
  &= -(\frac{1}{1+e^{-z}})^{2}\times(e^{-z})\times(-1) \\
  &= \frac{1}{1+e^{-z}}\times\frac{(1+e^{-z})-1}{1+e^{-z}} \\
  &= \sigma(z)[1-\sigma(z)] \\  
  \frac{d\mathcal{L}(a)}{da} &= \{-[ylog(a) + (1-y)log(1-a)]\}' \\
  &=-[\frac{y}{a}+\frac{1-y}{1-a}\times(-1)] \\
  &=-[\frac{y}{a}-\frac{1-y}{1-a}] 
\end{align*}</script><h4 id="matrix-derivative"><a href="#matrix-derivative" class="headerlink" title="matrix derivative"></a>matrix derivative</h4><p>Now, we’re ready to calculate matrix derivative. For matrix calculation, We’ll apply Einstein sumption convention. </p>
<p>We introduce a symbol $\epsilon_{ij}= 1$ when $j=i$, $\epsilon_{ij} = 0$ when $j\neq i$. we denote the row $i$, column $j$ eliment in matrix $M$ as $M_{ij}$. </p>
<script type="math/tex; mode=display">
\begin{align*} 
  \frac{\partial \mathcal{J}}{\partial b^{[2]}_{ij}} &= \frac{\partial \mathcal{J}}{\partial A^{[2]}_{kl}}\times\frac{\partial A^{[2]}_{kl}}{\partial Z^{[2]}_{mn}}\times\frac{\partial Z^{[2]}_{mn}}{\partial b^{[2]}_{ij}} \\
  &=-\frac{1}{m}(\frac{y_{kl}}{A^{[2]}_{kl}}-\frac{1-y_{kl}}{1-A^{[2]}_{kl}}) \times A^{[2]}_{kl}(1-A^{[2]}_{kl})\epsilon_{km}\epsilon_{ln} \times \epsilon_{mi}\epsilon_{nj} \\
  &=-\frac{1}{m}(\frac{y_{ij}}{A^{[2]}_{ij}}-\frac{1-y_{ij}}{1-A^{[2]}_{ij}}) \times A^{[2]}_{ij}(1-A^{[2]}_{ij}) \\
  &=-\frac{1}{m}[y_{ij}(1-A^{[2]}_{ij})-(1-y_{ij})A^{[2]}_{ij}] \\
  &=\frac{1}{m}(A^{[2]}_{ij}-y_{ij}) \\
\end{align*}</script><script type="math/tex; mode=display">
\begin{align*}
  \frac{\partial \mathcal{J}}{\partial W^{[2]}_{ij}} &= \frac{\partial \mathcal{J}}{\partial A^{[2]}_{kl}}\times\frac{\partial A^{[2]}_{kl}}{\partial Z^{[2]}_{mn}}\times\frac{\partial Z^{[2]}_{mn}}{\partial W^{[2]}_{ij}} \\
  &=-\frac{1}{m}(\frac{y_{kl}}{A^{[2]}_{kl}}-\frac{1-y_{kl}}{1-A^{[2]}_{kl}}) \times A^{[2]}_{kl}(1-A^{[2]}_{kl})\epsilon_{km}\epsilon_{ln} \times A^{[1]}_{jn}\epsilon_{mi} \\
  &=-\frac{1}{m}(\frac{y_{in}}{A^{[2]}_{in}}-\frac{1-y_{in}}{1-A^{[2]}_{in}}) \times A^{[2]}_{in}(1-A^{[2]}_{in}) \times A^{[1]}_{jn} \\
  &=-\frac{1}{m}[y_{in}(1-A^{[2]}_{in})-(1-y_{in})A^{[2]}_{in}] \times A^{[1]}_{jn} \\
  &=\frac{1}{m}(A^{[2]}_{in}-y_{in}) \times A^{[1]}_{jn} \\ 
\end{align*}</script><script type="math/tex; mode=display">
\begin{align*}
  \frac{\partial \mathcal{J}}{\partial b^{[1]}_{ij}} &= \frac{\partial \mathcal{J}}{\partial A^{[2]}_{kl}}\times\frac{\partial A^{[2]}_{kl}}{\partial Z^{[2]}_{mn}} \times \frac{\partial Z^{[2]}_{mn}}{\partial A^{[1]}_{gh}} \times \frac{\partial A^{[1]}_{gh}}{\partial Z^{[1]}_{op}}  \times \frac{\partial Z^{[1]}_{op}}{\partial b^{[1]}_{ij}}\\
  &=-\frac{1}{m}(\frac{y_{kl}}{A^{[2]}_{kl}}-\frac{1-y_{kl}}{1-A^{[2]}_{kl}}) \times A^{[2]}_{kl}(1-A^{[2]}_{kl})\epsilon_{km}\epsilon_{ln} \times W^{[2]}_{mg}\epsilon_{nh} \times A^{[1]}_{gh}(1-A^{[1]}_{gh})\epsilon_{go}\epsilon_{hp} \times \epsilon_{io}\epsilon_{jp}\\
  &=-\frac{1}{m}(\frac{y_{kj}}{A^{[2]}_{kj}}-\frac{1-y_{kj}}{1-A^{[2]}_{kj}}) \times A^{[2]}_{kj}(1-A^{[2]}_{kj}) \times W^{[2]}_{ki} \times A^{[1]}_{ij}(1-A^{[1]}_{ij})\\
  &=-\frac{1}{m}[y_{kj}(1-A^{[2]}_{kj})-(1-y_{kj})A^{[2]}_{kj}] \times W^{[2]}_{ki} \times A^{[1]}_{ij}(1-A^{[1]}_{ij}) \\
  &=\frac{1}{m}(A^{[2]}_{kj}-y_{kj}) \times W^{[2]}_{ki} \times A^{[1]}_{ij}(1-A^{[1]}_{ij}) \\
\end{align*}</script><script type="math/tex; mode=display">
\begin{align*}
  \frac{\partial \mathcal{J}}{\partial W^{[1]}_{ij}} &= \frac{\partial \mathcal{J}}{\partial A^{[2]}_{kl}}\times\frac{\partial A^{[2]}_{kl}}{\partial Z^{[2]}_{mn}} \times \frac{\partial Z^{[2]}_{mn}}{\partial A^{[1]}_{gh}} \times \frac{\partial A^{[1]}_{gh}}{\partial Z^{[1]}_{op}}  \times \frac{\partial Z^{[1]}_{op}}{\partial W^{[1]}_{ij}}\\
  &=-\frac{1}{m}(\frac{y_{kl}}{A^{[2]}_{kl}}-\frac{1-y_{kl}}{1-A^{[2]}_{kl}}) \times A^{[2]}_{kl}(1-A^{[2]}_{kl})\epsilon_{km}\epsilon_{ln} \times W^{[2]}_{mg}\epsilon_{nh} \times A^{[1]}_{gh}(1-A^{[1]}_{gh})\epsilon_{go}\epsilon_{hp} \times A^{[0]}_{jp}\epsilon_{io}\\
  &=-\frac{1}{m}(\frac{y_{kl}}{A^{[2]}_{kl}}-\frac{1-y_{kl}}{1-A^{[2]}_{kl}}) \times A^{[2]}_{kl}(1-A^{[2]}_{kl}) \times W^{[2]}_{ki} \times A^{[1]}_{il}(1-A^{[1]}_{il}) \times A^{[0]}_{jl}\\
  &=-\frac{1}{m}[y_{kl}(1-A^{[2]}_{kl})-(1-y_{kl})A^{[2]}_{kl}] \times W^{[2]}_{ki} \times A^{[1]}_{il}(1-A^{[1]}_{il}) \times A^{[0]}_{jl} \\
  &=\frac{1}{m}(A^{[2]}_{kl}-y_{kl}) \times W^{[2]}_{ki} \times A^{[1]}_{il}(1-A^{[1]}_{il}) \times A^{[0]}_{jl} \\
\end{align*}</script><p>Since we apply broadcasting, we have $b^{[l]}_{ij} = b^{[l]}_{ik}$ for any $i, j, k, l$. So we could denote $b^{[l]}_i = b^{[l]}_{ij}$. And all the derivative $db^{[l]}_{ij}$ could contribute(add up) to $db^{[l]}_{i}$. In other words, $db^{[l]}_{i} = \sum_{j=1}^{m}db^{[l]}_{ij}$</p>
<p>Based on Einstein sumption convention, above equation can be transfered as matrix equation:</p>
<script type="math/tex; mode=display">
\begin{align*}
  \frac{\partial \mathcal{J}}{\partial W^{[2]}} &= \frac{1}{m}(A^{[2]}-Y) \cdot (A^{[1]})^T \\
  \frac{\partial \mathcal{J}}{\partial b^{[2]}} &= \frac{1}{m}np.sum(A^{[2]}-Y, axis=1, keepdims=True)\\
  \frac{\partial \mathcal{J}}{\partial W^{[1]}} &= \frac{1}{m}[(W^{[2]})^T \cdot (A^{[2]}-Y)] * [A^{[1]} * (1-A^{[1]})] \cdot (A^{[0]})^T\\
  \frac{\partial \mathcal{J}}{\partial b^{[1]}} &= \frac{1}{m}np.sum([(W^{[2]})^T \cdot (A^{[2]}-Y)] * [A^{[1]} * (1-A^{[1]}]), axis=1, keepdims=True) \\
\end{align*}</script><h4 id="python-code"><a href="#python-code" class="headerlink" title="python code"></a>python code</h4><p>For python code variable name convenient, we denote:</p>
<script type="math/tex; mode=display">
\begin{align*}
W2 &= W^{[2]} \\
b2 &= b^{[2]} \\
W1 &= W^{[1]} \\
b1 &= b^{[1]} \\
dW2 &= \frac{\partial \mathcal{J}}{\partial W^{[2]}} \\
db2 &= \frac{\partial \mathcal{J}}{\partial b^{[2]}} \\
dW1 &= \frac{\partial \mathcal{J}}{\partial W^{[1]}} \\
db1 &= \frac{\partial \mathcal{J}}{\partial b^{[1]}} \\
\end{align*}</script><p>Then the python code for getting them is:</p>
<pre><code>db2 = 1.0/m * np.sum(A2 - Y, axis=1, keepdims=True)
dw2 = 1.0/m * np.dot(A2 - Y, A1.T)
db1 = 1.0/m * np.sum(np.dot(W2.T, A2 - Y) * (A1 * (1 - A1)), axis=1, keepdims=True)
dw1 = 1.0/m * np.dot(np.dot(W2.T, A2 - Y) * (A1 * (1 - A1)), X.T)
</code></pre><h4 id="cache-for-computational-benefit"><a href="#cache-for-computational-benefit" class="headerlink" title="cache for computational benefit"></a>cache for computational benefit</h4><p>From above, there are many <strong>repetitive</strong> calculation. such as $A^{[2]} - Y$, which appear everywhere. We could cache such frequently used values to save calculation. Actually, from the equation above it’s easy to see that $\frac{\partial \mathcal{J}}{\partial Z^{[1]}}$ and $\frac{\partial \mathcal{J}}{\partial Z^{[2]}}$ are the variables worth to cache. Because:</p>
<script type="math/tex; mode=display">
\begin{align*}
  \frac{\partial \mathcal{J}}{\partial W^{[2]}} &= \frac{\partial \mathcal{J}}{\partial Z^{[2]}} \cdot \frac{\partial Z^{[2]}}{\partial W^{[2]}} \\
  \frac{\partial \mathcal{J}}{\partial b^{[2]}} &= \frac{\partial \mathcal{J}}{\partial Z^{[2]}} \cdot \frac{\partial Z^{[2]}}{\partial b^{[2]}} \\
  \frac{\partial \mathcal{J}}{\partial W^{[1]}} &= \frac{\partial \mathcal{J}}{\partial Z^{[1]}} \cdot \frac{\partial Z^{[1]}}{\partial W^{[1]}} \\
  \frac{\partial \mathcal{J}}{\partial b^{[1]}} &= \frac{\partial \mathcal{J}}{\partial Z^{[1]}} \cdot \frac{\partial Z^{[1]}}{\partial b^{[1]}} \\
\end{align*}</script><p>It’s easy to know, that:</p>
<script type="math/tex; mode=display">
\begin{align*}
  \frac{\partial \mathcal{J}}{\partial Z^{[2]}} &= \frac{1}{m}(A^{[2]}-Y) \\
  \frac{\partial \mathcal{J}}{\partial Z^{[1]}} &= \frac{1}{m}[(W^{[2]})^T \cdot (A^{[2]}-Y)] * [A^{[1]} * (1-A^{[1]})] \\
  &= [(W^{[2]})^T \cdot \frac{\partial \mathcal{J}}{\partial Z^{[2]}}] * [A^{[1]} * (1-A^{[1]})] \\
\end{align*}</script><p>So:</p>
<script type="math/tex; mode=display">
\begin{align*}  
  \frac{\partial \mathcal{J}}{\partial b^{[2]}} &= np.sum(\frac{\partial \mathcal{J}}{\partial Z^{[2]}}, axis=1, keepdims=True)\\
  \frac{\partial \mathcal{J}}{\partial W^{[2]}} &= \frac{\partial \mathcal{J}}{\partial Z^{[2]}}\cdot (A^{[1]})^T \\  
  \frac{\partial \mathcal{J}}{\partial b^{[1]}} &= np.sum(\frac{\partial \mathcal{J}}{\partial Z^{[1]}}, axis=1, keepdims=True) \\
  \frac{\partial \mathcal{J}}{\partial W^{[1]}} &= \frac{\partial \mathcal{J}}{\partial Z^{[1]}} \cdot (A^{[0]})^T\\
\end{align*}</script><p>So the pytho code with cache is:</p>
<pre><code>dZ2 = (1.0/m) * (A2 - Y)
db2 = np.sum(dZ2, axis=1, keepdims=True)
dw2 = np.dot(dZ2, A1.T)
dZ1 = np.dot(W2.T, dZ2) * (A1 * (1 - A1)
db1 = np.sum(dZ1, axis=1, keepdims=True)
dw1 = np.dot(dZ1, X.T)
</code></pre><h3 id="Extend-layer-number-and-hidden-unit-number"><a href="#Extend-layer-number-and-hidden-unit-number" class="headerlink" title="Extend layer number and hidden unit number"></a>Extend layer number and hidden unit number</h3><h4 id="unit-number"><a href="#unit-number" class="headerlink" title="unit number"></a>unit number</h4><p>In our above deduction, we actually not limit(hard code) the layer unit number, we only apply $n^{[l]}$ to represent the unit number for layer $l$. Even for the input layer $X$ and output layer $Y$. So you can choose any unit number you like, our equation will still apply to them.</p>
<p>Just to address that, if you choose more that one unit for output layer $Y$, then it could be multi-classification system other than binary classification. But then you may need to redefine the cost function and lost function(original may not work well). If you want to extend output layer unit and redefine loss function $\mathcal{L}$, just remember to replace $\frac{\partial \mathcal{J}}{\partial Z^{[n]}}$ carefully in the equation, then all will work fine. </p>
<h4 id="layer-number"><a href="#layer-number" class="headerlink" title="layer number"></a>layer number</h4><p>In our above deduction, we limit the layer number to be 2(exclude input layer). But we could do the same calculation for 3, 4 or n layer system. It’s not hard to get the formula for n layer network by mathematical induction. In following, we denote $A^{[0]}=X$, $A^{[n]}=\hat Y$, $Y$ to be the real label from data. And assuming only one unit in output layer $Y$, so apply original $\mathcal{L}$ and $\mathcal{J}$. </p>
<h5 id="formula"><a href="#formula" class="headerlink" title="formula"></a>formula</h5><p><strong>forward propogation</strong>:</p>
<script type="math/tex; mode=display">
\begin{align*}
 Z^{[1]} &= W^{[1]} \cdot A^{[0]}+b^{[1]} \\
 A^{[1]} &= \sigma(Z^{[1]}) \\
 Z^{[2]} &= W^{[2]} \cdot A^{[1]}+b^{[2]} \\
 A^{[2]} &= \sigma(Z^{[2]}) \\
 ... \\
 Z^{[n]} &= W^{[n]} \cdot A^{[n-1]}+b^{[n]} \\
 A^{[n]} &= \sigma(Z^{[n]})
\end{align*}</script><p>And final cost function: </p>
<script type="math/tex; mode=display">\mathcal{J}=-\frac{1}{m}\times[YlogA^{[n]} + (1-Y)log(1-(A^{[n]})^T)]</script><p><strong>backward propogation</strong>:</p>
<script type="math/tex; mode=display">
\begin{align*}
  \frac{\partial \mathcal{J}}{\partial Z^{[n]}} &= \frac{1}{m}(A^{[n]}-Y) \\
  \frac{\partial \mathcal{J}}{\partial b^{[n]}} &= np.sum(\frac{\partial \mathcal{J}}{\partial Z^{[n]}}, axis=1, keepdims=True)\\
  \frac{\partial \mathcal{J}}{\partial W^{[n]}} &= \frac{\partial \mathcal{J}}{\partial Z^{[n]}}\cdot (A^{[n-1]})^T \\  
  \frac{\partial \mathcal{J}}{\partial Z^{[n-1]}} &= [(W^{[n]})^T \cdot \frac{\partial \mathcal{J}}{\partial Z^{[n]}}] * [A^{[n-1]} * (1-A^{[n-1]})] \\
   \frac{\partial \mathcal{J}}{\partial b^{[n-1]}} &= np.sum(\frac{\partial \mathcal{J}}{\partial Z^{[n-1]}}, axis=1, keepdims=True)\\
  \frac{\partial \mathcal{J}}{\partial W^{[n-1]}} &= \frac{\partial \mathcal{J}}{\partial Z^{[n-1]}}\cdot (A^{[n-2]})^T \\  
  ... \\
  \frac{\partial \mathcal{J}}{\partial Z^{[k]}} &= [(W^{[k+1]})^T \cdot \frac{\partial \mathcal{J}}{\partial Z^{[k+1]}}] * [A^{[k]} * (1-A^{[k]})] \\
  \frac{\partial \mathcal{J}}{\partial b^{[k]}} &= np.sum(\frac{\partial \mathcal{J}}{\partial Z^{[k]}}, axis=1, keepdims=True)\\
  \frac{\partial \mathcal{J}}{\partial W^{[k]}} &= \frac{\partial \mathcal{J}}{\partial Z^{[k]}}\cdot (A^{[k-1]})^T \\ 
  ... \\
  \frac{\partial \mathcal{J}}{\partial Z^{[1]}} &= [(W^{[2]})^T \cdot \frac{\partial \mathcal{J}}{\partial Z^{[2]}}] * [A^{[1]} * (1-A^{[1]})] \\
  \frac{\partial \mathcal{J}}{\partial b^{[1]}} &= np.sum(\frac{\partial \mathcal{J}}{\partial Z^{[1]}}, axis=1, keepdims=True) \\
  \frac{\partial \mathcal{J}}{\partial W^{[1]}} &= \frac{\partial \mathcal{J}}{\partial Z^{[1]}} \cdot (A^{[0]})^T\\
\end{align*}</script><h5 id="python-code-1"><a href="#python-code-1" class="headerlink" title="python code"></a>python code</h5><p>We assume provided W=[0, W1, W2, …, Wn], b=[0, b1, b2, …, bn].</p>
<p><strong>import package</strong>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">np.random.seed(<span class="number">1</span>) <span class="comment"># set a seed so that the results are consistent</span></span><br></pre></td></tr></table></figure></p>
<p><strong>forward propogation</strong>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sigmoid function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"><span class="comment"># forward to get A</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(X, b, W, layer_number)</span>:</span> </span><br><span class="line">    A0 = X</span><br><span class="line">    n = layer_number</span><br><span class="line">    A = [A0]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>): </span><br><span class="line">        A.append(sigmoid(np.dot(W[i], A[i<span class="number">-1</span>]) + b[i])) <span class="comment">#</span></span><br><span class="line">    <span class="keyword">return</span> A <span class="comment"># A = [A0, A1, A2 ... An]</span></span><br><span class="line"><span class="comment"># calculate cost based on A</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_cost</span><span class="params">(An, Y)</span>:</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># number of examples</span></span><br><span class="line">    cost = -(<span class="number">1.0</span>)/m * np.sum(np.multiply(np.log(An), Y) + np.multiply((<span class="number">1</span> - Y), np.log(<span class="number">1</span> - An)))</span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># makes sure cost is the dimension we expect. like [[17]] into 17</span></span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure></p>
<p><strong>backward propogation:</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># backward to get db, dW</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(W, A, Y, layer_number)</span>:</span></span><br><span class="line">    dZ = <span class="keyword">None</span> <span class="comment">#for keeping the latest dZi</span></span><br><span class="line">    db = [<span class="number">0</span>]</span><br><span class="line">    dW = [<span class="number">0</span>]</span><br><span class="line">    n = layer_number</span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">if</span> i == n:</span><br><span class="line">            dZ = <span class="number">1.0</span>/m * (A[i] - Y) <span class="comment"># since A=[A0, A1, ..An], so An = A[n]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dZ = np.dot(W[i+<span class="number">1</span>].T, dZ) * (A[i] * (<span class="number">1</span> - A[i])) <span class="comment"># giving W=[0, W1, W2, ... Wn], so Wn = W[n]</span></span><br><span class="line">        db.insert(<span class="number">1</span>, np.sum(dZ, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)) <span class="comment"># db=[0, dbi, dbi+1, ..., dbn]</span></span><br><span class="line">        dW.insert(<span class="number">1</span>, np.dot(dZ, A[i<span class="number">-1</span>].T)) <span class="comment"># dW=[0, dWi, dWi+1, ..., dWn]</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> db, dW <span class="comment"># now db=[0, db1, db2, ..., dbn], dW=[0, dW1, dW2, ..., dWn]</span></span><br><span class="line"><span class="comment"># update d and W</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_params</span><span class="params">(db, dW, b, W, learning_rate, layer_number)</span>:</span></span><br><span class="line">    n = layer_number</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">        b[i] = b[i] - learning_rate * db[i]</span><br><span class="line">        W[i] = W[i] - learning_rate * dW[i]</span><br><span class="line">    <span class="keyword">return</span> b, W </span><br></pre></td></tr></table></figure></p>
<p><strong>bulid model</strong>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialize d and W</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inital_params</span><span class="params">(X, Y, hidden_layer_units)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">2</span>)</span><br><span class="line">    b = [<span class="number">0</span>]</span><br><span class="line">    W = [<span class="number">0</span>]</span><br><span class="line">    n_last = X.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># for all hidden layer, saying n[1], n[2], ... n[l-1]</span></span><br><span class="line">    <span class="keyword">for</span> unit_number <span class="keyword">in</span> hidden_layer_units: <span class="comment"># hidden_layer_units = [n[1], n[2], ..., n[l-1]]</span></span><br><span class="line">        n_current = unit_number      </span><br><span class="line">        b.append(np.zeros((n_current, <span class="number">1</span>)))</span><br><span class="line">        W.append(np.random.randn(n_current, n_last)*<span class="number">0.01</span>)</span><br><span class="line">        n_last = n_current</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># for last ouput layer n[l]</span></span><br><span class="line">    b.append(np.zeros((Y.shape[<span class="number">0</span>], <span class="number">1</span>)))</span><br><span class="line">    W.append(np.random.randn(Y.shape[<span class="number">0</span>], n_last)*<span class="number">0.01</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> b, W</span><br><span class="line"></span><br><span class="line"><span class="comment"># build model by put all together</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, hidden_layer_units=[<span class="number">4</span>], learning_rate=<span class="number">0.005</span>, iterate_times=<span class="number">1000</span>, print_cost=True)</span>:</span></span><br><span class="line">    layer_number = len(hidden_layer_units)+<span class="number">1</span></span><br><span class="line">    b, W = inital_params(X, Y, hidden_layer_units)</span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iterate_times):</span><br><span class="line">        A = forward(X, b, W, layer_number)   </span><br><span class="line">        db, dW = backward(W, A, Y, layer_number)</span><br><span class="line">        b, W = update_params(db, dW, b, W, learning_rate, layer_number)</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i%<span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            cost = cal_cost(A[layer_number], Y)</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            print(<span class="string">"cost after "</span> + str(i) + <span class="string">" iterations: "</span> + str(cost))</span><br><span class="line">            </span><br><span class="line">    model_paras = &#123;<span class="string">'b'</span>:b,</span><br><span class="line">                   <span class="string">'W'</span>:W,</span><br><span class="line">                   <span class="string">'costs'</span>:costs&#125;</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> model_paras</span><br><span class="line"><span class="comment"># predict Y based on model, Y is either 0 or 1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X, b, W)</span>:</span></span><br><span class="line">    layer_number = len(b)<span class="number">-1</span></span><br><span class="line">    A = forward(X, b, W, layer_number)</span><br><span class="line">    Y = np.round(A[<span class="number">-1</span>])</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"><span class="comment"># evaluate model with visulization</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(X, Y, b, W)</span>:</span>    </span><br><span class="line">    predictions = predict(X, b, W)</span><br><span class="line">    accuracy = float((np.dot(Y, predictions.T) + np.dot(<span class="number">1</span> - Y, <span class="number">1</span> - predictions.T)) / float(Y.size) * <span class="number">100</span>)</span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br></pre></td></tr></table></figure></p>
<h3 id="Case-Application"><a href="#Case-Application" class="headerlink" title="Case Application"></a>Case Application</h3><p>The model we build is suitable for all binary classification problem. And it’s structure is configurable in hidden_layer parameter. Asuming we could get training data from load_dataset() function, then the regular way to apply our model is:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set parameter for learning, those are the super parameters you can tune.</span></span><br><span class="line">hidden_layer_units = [<span class="number">4</span>]</span><br><span class="line">learning_rate = <span class="number">1.2</span></span><br><span class="line">iterate_times = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load training data</span></span><br><span class="line">X, Y = load_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># build model</span></span><br><span class="line">model_paras = model(X, Y, hidden_layer_units, learning_rate, iterate_times)</span><br><span class="line">b = model_paras[<span class="string">'b'</span>]</span><br><span class="line">W = model_paras[<span class="string">'W'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># evaludate model</span></span><br><span class="line">accuracy = evaluate(X, Y, b, W)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy: %d '</span> % accuracy + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="Other-framework"><a href="#Other-framework" class="headerlink" title="Other framework"></a>Other framework</h3><p>You could implement the equivalent model with framework easily. two of the frameworks are tensorflow and keras.</p>
<h4 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h4><p>the key functions to implement the same model in tensorflow are like following. As you can see from the code, all you need to care is the forward part, tensorflow would take care of the backward part automaticlly since it stored the flow graph of all the units as sessions.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">        tf.set_random_seed(<span class="number">1</span>)                   <span class="comment"># so that your "random" numbers match ours</span></span><br><span class="line">        </span><br><span class="line">        W1 = tf.get_variable(<span class="string">"W1"</span>, [<span class="number">25</span>, <span class="number">12288</span>], initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">        b1 = tf.get_variable(<span class="string">"b1"</span>, [<span class="number">25</span>, <span class="number">1</span>], initializer=tf.zeros_initializer())</span><br><span class="line">        W2 = tf.get_variable(<span class="string">"W2"</span>, [<span class="number">12</span>, <span class="number">25</span>], initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">        b2 = tf.get_variable(<span class="string">"b2"</span>, [<span class="number">12</span>, <span class="number">1</span>], initializer=tf.zeros_initializer())</span><br><span class="line">        W3 = tf.get_variable(<span class="string">"W3"</span>, [<span class="number">6</span>, <span class="number">12</span>], initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">        b3 = tf.get_variable(<span class="string">"b3"</span>, [<span class="number">6</span>, <span class="number">1</span>], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">        parameters = &#123;<span class="string">"W1"</span>: W1,<span class="string">"b1"</span>: b1,<span class="string">"W2"</span>: W2,<span class="string">"b2"</span>: b2,<span class="string">"W3"</span>: W3,<span class="string">"b3"</span>: b3&#125;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">        <span class="comment"># Retrieve the parameters from the dictionary "parameters" </span></span><br><span class="line">        W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">        b1 = parameters[<span class="string">'b1'</span>]</span><br><span class="line">        W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">        b2 = parameters[<span class="string">'b2'</span>]</span><br><span class="line">        W3 = parameters[<span class="string">'W3'</span>]</span><br><span class="line">        b3 = parameters[<span class="string">'b3'</span>]</span><br><span class="line">    </span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:</span></span><br><span class="line">        Z1 = tf.add(tf.matmul(W1, X), b1)                                              <span class="comment"># Z1 = np.dot(W1, X) + b1</span></span><br><span class="line">        A1 = tf.nn.relu(Z1)                                              <span class="comment"># A1 = relu(Z1)</span></span><br><span class="line">        Z2 = tf.add(tf.matmul(W2, A1), b2)                                              <span class="comment"># Z2 = np.dot(W2, a1) + b2</span></span><br><span class="line">        A2 = tf.nn.relu(Z2)                                              <span class="comment"># A2 = relu(Z2)</span></span><br><span class="line">        Z3 = tf.add(tf.matmul(W3, A2), b3)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> Z3</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Z3, Y)</span>:</span>  </span><br><span class="line">        <span class="comment"># to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)</span></span><br><span class="line">        logits = tf.transpose(Z3)</span><br><span class="line">        labels = tf.transpose(Y)</span><br><span class="line">    </span><br><span class="line">        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, learning_rate = <span class="number">0.0001</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">          num_epochs = <span class="number">1500</span>, minibatch_size = <span class="number">32</span>, print_cost = True)</span>:</span>    </span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Create Placeholders of shape (n_x, n_y)</span></span><br><span class="line">        X, Y = create_placeholders(n_x, n_y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize parameters</span></span><br><span class="line">        parameters = initialize_parameters()</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Forward propagation: Build the forward propagation in the tensorflow graph</span></span><br><span class="line">        Z3 = forward_propagation(X, parameters)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Cost function: Add cost function to tensorflow graph</span></span><br><span class="line">        cost = compute_cost(Z3, Y)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.</span></span><br><span class="line">        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Initialize all the variables</span></span><br><span class="line">        init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Start the session to compute the tensorflow graph</span></span><br><span class="line">        <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:        </span><br><span class="line">            <span class="comment"># Run the initialization</span></span><br><span class="line">            sess.run(init)</span><br><span class="line">            _ , cost = sess.run([optimizer, cost], feed_dict=&#123;X:X, Y:Y&#125;)</span><br><span class="line">            </span><br><span class="line">                </span><br><span class="line">            <span class="comment"># lets save the parameters in a variable</span></span><br><span class="line">            parameters = sess.run(parameters)</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Parameters have been trained!"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></p>
<h4 id="keras"><a href="#keras" class="headerlink" title="keras"></a>keras</h4><p>Keras is a higher level framework than tensorflow, it provided many more useful functions for implementing CNN and RNN. Basicly, you could apply it to build a model with only few line of code as it take care of both forward and backward part for common models, it’s basic block is layer, you could choose the provided layer to stack your model quickly.</p>
<p>The key code for building the same model we did before is:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense, Activation</span><br><span class="line"></span><br><span class="line">X_input = Input(input_shape)</span><br><span class="line">X_hidden = Dense(<span class="number">4</span>, activation=<span class="string">'sigmoid'</span>, name=<span class="string">'fullycon1'</span>)(X_input)</span><br><span class="line">Y = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>, name=<span class="string">'fullycon2'</span>)(X_hidden)</span><br><span class="line">model = Model(inputs=X_input, outputs=Y, name=<span class="string">'fullycon_model'</span>)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'mean_squared_error'</span>, metrics=[<span class="string">"accuracy"</span>])</span><br><span class="line">model.fit(x=X_train, y=Y_train, epochs=<span class="number">10</span>, batch_size=<span class="number">100</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="Questions"><a href="#Questions" class="headerlink" title="Questions:"></a>Questions:</h2><h2 id="History"><a href="#History" class="headerlink" title="History:"></a>History:</h2><ul>
<li><em>2019-02-06</em>: create post and draft based on my notebook.</li>
</ul>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/neural-network/" rel="tag"># neural network</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04/10/conjugate-gradient-optimization-method/" rel="next" title="Conjugate Gradient Method">
                <i class="fa fa-chevron-left"></i> Conjugate Gradient Method
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2021/02/25/write-blog-with-hexo/" rel="prev" title="Write Blog With Hexo">
                Write Blog With Hexo <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Don</p>
              <p class="site-description motion-element" itemprop="description">学习，记录，交流，分享...</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">24</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">27</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Content"><span class="nav-number">2.</span> <span class="nav-text">Content:</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Structure"><span class="nav-number">2.1.</span> <span class="nav-text">Structure</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Conventions"><span class="nav-number">2.2.</span> <span class="nav-text">Conventions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Forward-Propogation"><span class="nav-number">2.3.</span> <span class="nav-text">Forward Propogation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Backward-Propogation"><span class="nav-number">2.4.</span> <span class="nav-text">Backward Propogation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#chain-rule"><span class="nav-number">2.4.1.</span> <span class="nav-text">chain rule</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#derivative-for-sigmoid-function-and-loss-function"><span class="nav-number">2.4.2.</span> <span class="nav-text">derivative for sigmoid function and loss function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#matrix-derivative"><span class="nav-number">2.4.3.</span> <span class="nav-text">matrix derivative</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#python-code"><span class="nav-number">2.4.4.</span> <span class="nav-text">python code</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cache-for-computational-benefit"><span class="nav-number">2.4.5.</span> <span class="nav-text">cache for computational benefit</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Extend-layer-number-and-hidden-unit-number"><span class="nav-number">2.5.</span> <span class="nav-text">Extend layer number and hidden unit number</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#unit-number"><span class="nav-number">2.5.1.</span> <span class="nav-text">unit number</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#layer-number"><span class="nav-number">2.5.2.</span> <span class="nav-text">layer number</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#formula"><span class="nav-number">2.5.2.1.</span> <span class="nav-text">formula</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#python-code-1"><span class="nav-number">2.5.2.2.</span> <span class="nav-text">python code</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Case-Application"><span class="nav-number">2.6.</span> <span class="nav-text">Case Application</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-framework"><span class="nav-number">2.7.</span> <span class="nav-text">Other framework</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tensorflow"><span class="nav-number">2.7.1.</span> <span class="nav-text">tensorflow</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#keras"><span class="nav-number">2.7.2.</span> <span class="nav-text">keras</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Questions"><span class="nav-number">3.</span> <span class="nav-text">Questions:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#History"><span class="nav-number">4.</span> <span class="nav-text">History:</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Don</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v6.7.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/schemes/muse.js?v=6.7.0"></script>




  
  <script src="/js/src/scrollspy.js?v=6.7.0"></script>
<script src="/js/src/post-details.js?v=6.7.0"></script>



  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  


  


  





  

  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow: auto hidden;
}
</style>

    
  


  

  

  

  

  

  

  

  

</body>
</html>
